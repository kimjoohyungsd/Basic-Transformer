Above code is basic practice code for implementing Transformer blocks in Attention is all you need
